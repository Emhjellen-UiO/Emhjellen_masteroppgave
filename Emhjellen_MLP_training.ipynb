{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MLP-models\n",
    "\n",
    "Linn Alexandra Emhjellen, 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,confusion_matrix\n",
    "\n",
    "from sklearn.metrics import recall_score,roc_curve,auc\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading calibration set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_excel('ML_training_features.xlsx')\n",
    "train_y = pd.read_excel('ML_training_target.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_X = pd.read_excel('ML_validation_features.xlsx')\n",
    "validation_y = pd.read_excel('ML_validation_target.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_y = validation_y['ReleaseArea']\n",
    "train_y = train_y['ReleaseArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optimized hyperparameters from RandomSearch with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_MLP = pd.read_excel('MLP_best_params_RandomSearch.xlsx')\n",
    "best_params_MLP = best_params_MLP.drop(columns = 'Unnamed: 0')\n",
    "best_param_grid = best_params_MLP.to_dict(orient = 'records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature combinations\n",
    "p1 = ['Slope']\n",
    "\n",
    "p2 = ['Slope','Elevation']\n",
    "\n",
    "p3 = ['Slope','North','East','North East','North West','South','South East','South West','West']\n",
    "\n",
    "p4 = ['Slope','Elevation','Plan_curv','Profile_curv','TRI','Distance_to_roads']\n",
    "\n",
    "p5 = ['Slope','Elevation','Plan_curv','Profile_curv','TRI','Flow_dir','Flow_acc','Distance_to_roads']\n",
    "\n",
    "p6 = ['Slope','Elevation','Plan_curv','Profile_curv','TRI']\n",
    "\n",
    "p7 = ['Elevation','North','East','North East','North West','South','South East','South West','West','Plan_curv','Profile_curv','TRI','Flow_dir','Flow_acc','Distance_to_roads']\n",
    "\n",
    "p8 = ['Slope','Elevation','North','East','North East','North West','South','South East','South West','West','Plan_curv','Profile_curv','TRI','Flow_dir','Flow_acc','Distance_to_roads',\n",
    "      'Granite','Granodiorite','Tonalite','Trondhjemite','Syenite','Monzonite','Monzodiorite','Quartz diorite','Diorite','Gabbro','Norite','Peridotite','Pyroksenite','Charnockite','Mangerite','Anorthosite','Mafic dyke (Diabase, Dolerite)','Pegmatite/aplite','Felsic volcanic rock','Rhyolite','Dacite','Intermediate volcanic rock','Andesite','Mafic volcanic rock','Basalt',\n",
    "                  'Pyroclastic rock','Volcanic breccia','Siltstone','Sandstone','Greywacke','Arkose','Konglomerate','Sedimentary breccia','Limestone','Tuffite','Shale','Phyllite','Mica schist','Garnet mica schist','Calcareous phyllite','Calcareous mica schist','Amphibole schist','Graphitic schist','Calcite marble',\n",
    "                 'Metasandstone','Metagreywacke','Meta-arkose','Quartzite','Quartz schist','Mica gneiss','Calc-silicate rock','Amphibole gneiss','Granitic gneiss','Granodioritic gneiss','Tonalitic gneiss','Quartz dioritic gneiss','Monzonitic gneiss','Dioritic gneis','Orthopyroxene gneiss','Migmatite','Augengneiss',\n",
    "                    'Banded gneiss','Greenschist','Greenstone','Amphibolite','Metagabbro','Eclogite','Serpentinite','Mylonite/Phyllonite','Cataclasite']\n",
    "\n",
    "feature_combinations = [p1,p2,p3,p4,p5,p6,p7,p8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.49329313\n",
      "Iteration 2, loss = 0.25705141\n",
      "Iteration 3, loss = 0.18584060\n",
      "Iteration 4, loss = 0.16627930\n",
      "Iteration 5, loss = 0.16033727\n",
      "Iteration 6, loss = 0.15839359\n",
      "Iteration 7, loss = 0.15758800\n",
      "Iteration 8, loss = 0.15718740\n",
      "Iteration 9, loss = 0.15686088\n",
      "Iteration 10, loss = 0.15666376\n",
      "Iteration 11, loss = 0.15652130\n",
      "Iteration 12, loss = 0.15639366\n",
      "Iteration 13, loss = 0.15629016\n",
      "Iteration 14, loss = 0.15629755\n",
      "Iteration 15, loss = 0.15615804\n",
      "Iteration 16, loss = 0.15619507\n",
      "Iteration 17, loss = 0.15616823\n",
      "Iteration 18, loss = 0.15612967\n",
      "Iteration 19, loss = 0.15618076\n",
      "Iteration 20, loss = 0.15615905\n",
      "Iteration 21, loss = 0.15602008\n",
      "Iteration 22, loss = 0.15617861\n",
      "Iteration 23, loss = 0.15607845\n",
      "Iteration 24, loss = 0.15613335\n",
      "Iteration 25, loss = 0.15618914\n",
      "Iteration 26, loss = 0.15611393\n",
      "Iteration 27, loss = 0.15606942\n",
      "Iteration 28, loss = 0.15602103\n",
      "Iteration 29, loss = 0.15609924\n",
      "Iteration 30, loss = 0.15611314\n",
      "Iteration 31, loss = 0.15608958\n",
      "Iteration 32, loss = 0.15614109\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52433151\n",
      "Iteration 2, loss = 0.28399034\n",
      "Iteration 3, loss = 0.19399312\n",
      "Iteration 4, loss = 0.16984064\n",
      "Iteration 5, loss = 0.16222169\n",
      "Iteration 6, loss = 0.15942585\n",
      "Iteration 7, loss = 0.15808849\n",
      "Iteration 8, loss = 0.15728959\n",
      "Iteration 9, loss = 0.15671450\n",
      "Iteration 10, loss = 0.15626527\n",
      "Iteration 11, loss = 0.15588161\n",
      "Iteration 12, loss = 0.15564939\n",
      "Iteration 13, loss = 0.15536143\n",
      "Iteration 14, loss = 0.15513520\n",
      "Iteration 15, loss = 0.15509916\n",
      "Iteration 16, loss = 0.15490460\n",
      "Iteration 17, loss = 0.15484107\n",
      "Iteration 18, loss = 0.15476605\n",
      "Iteration 19, loss = 0.15474231\n",
      "Iteration 20, loss = 0.15467232\n",
      "Iteration 21, loss = 0.15463524\n",
      "Iteration 22, loss = 0.15463161\n",
      "Iteration 23, loss = 0.15456403\n",
      "Iteration 24, loss = 0.15455025\n",
      "Iteration 25, loss = 0.15461401\n",
      "Iteration 26, loss = 0.15451101\n",
      "Iteration 27, loss = 0.15449671\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50586640\n",
      "Iteration 2, loss = 0.27903880\n",
      "Iteration 3, loss = 0.20341648\n",
      "Iteration 4, loss = 0.17662696\n",
      "Iteration 5, loss = 0.16576619\n",
      "Iteration 6, loss = 0.16104670\n",
      "Iteration 7, loss = 0.15898948\n",
      "Iteration 8, loss = 0.15793319\n",
      "Iteration 9, loss = 0.15734918\n",
      "Iteration 10, loss = 0.15697323\n",
      "Iteration 11, loss = 0.15684745\n",
      "Iteration 12, loss = 0.15661901\n",
      "Iteration 13, loss = 0.15650445\n",
      "Iteration 14, loss = 0.15641246\n",
      "Iteration 15, loss = 0.15636716\n",
      "Iteration 16, loss = 0.15630972\n",
      "Iteration 17, loss = 0.15620840\n",
      "Iteration 18, loss = 0.15621011\n",
      "Iteration 19, loss = 0.15616230\n",
      "Iteration 20, loss = 0.15616164\n",
      "Iteration 21, loss = 0.15624596\n",
      "Iteration 22, loss = 0.15613144\n",
      "Iteration 23, loss = 0.15619193\n",
      "Iteration 24, loss = 0.15610064\n",
      "Iteration 25, loss = 0.15626796\n",
      "Iteration 26, loss = 0.15619315\n",
      "Iteration 27, loss = 0.15613447\n",
      "Iteration 28, loss = 0.15605806\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34460575\n",
      "Iteration 2, loss = 0.20628313\n",
      "Iteration 3, loss = 0.17168116\n",
      "Iteration 4, loss = 0.16187290\n",
      "Iteration 5, loss = 0.15846432\n",
      "Iteration 6, loss = 0.15723423\n",
      "Iteration 7, loss = 0.15655288\n",
      "Iteration 8, loss = 0.15616655\n",
      "Iteration 9, loss = 0.15592711\n",
      "Iteration 10, loss = 0.15573754\n",
      "Iteration 11, loss = 0.15566896\n",
      "Iteration 12, loss = 0.15553550\n",
      "Iteration 13, loss = 0.15549204\n",
      "Iteration 14, loss = 0.15544292\n",
      "Iteration 15, loss = 0.15542823\n",
      "Iteration 16, loss = 0.15541292\n",
      "Iteration 17, loss = 0.15538259\n",
      "Iteration 18, loss = 0.15535990\n",
      "Iteration 19, loss = 0.15537308\n",
      "Iteration 20, loss = 0.15534541\n",
      "Iteration 21, loss = 0.15531703\n",
      "Iteration 22, loss = 0.15528286\n",
      "Iteration 23, loss = 0.15526436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40462022\n",
      "Iteration 2, loss = 0.24108111\n",
      "Iteration 3, loss = 0.18803139\n",
      "Iteration 4, loss = 0.16925378\n",
      "Iteration 5, loss = 0.16186674\n",
      "Iteration 6, loss = 0.15878432\n",
      "Iteration 7, loss = 0.15728547\n",
      "Iteration 8, loss = 0.15653268\n",
      "Iteration 9, loss = 0.15604185\n",
      "Iteration 10, loss = 0.15578172\n",
      "Iteration 11, loss = 0.15560608\n",
      "Iteration 12, loss = 0.15540907\n",
      "Iteration 13, loss = 0.15526325\n",
      "Iteration 14, loss = 0.15518721\n",
      "Iteration 15, loss = 0.15510786\n",
      "Iteration 16, loss = 0.15504654\n",
      "Iteration 17, loss = 0.15504654\n",
      "Iteration 18, loss = 0.15501531\n",
      "Iteration 19, loss = 0.15494255\n",
      "Iteration 20, loss = 0.15498039\n",
      "Iteration 21, loss = 0.15495413\n",
      "Iteration 22, loss = 0.15495857\n",
      "Iteration 23, loss = 0.15492054\n",
      "Iteration 24, loss = 0.15494339\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51492758\n",
      "Iteration 2, loss = 0.29019487\n",
      "Iteration 3, loss = 0.21036998\n",
      "Iteration 4, loss = 0.18099555\n",
      "Iteration 5, loss = 0.16889842\n",
      "Iteration 6, loss = 0.16369961\n",
      "Iteration 7, loss = 0.16156390\n",
      "Iteration 8, loss = 0.16055444\n",
      "Iteration 9, loss = 0.16017478\n",
      "Iteration 10, loss = 0.15997587\n",
      "Iteration 11, loss = 0.15985139\n",
      "Iteration 12, loss = 0.15978691\n",
      "Iteration 13, loss = 0.15973534\n",
      "Iteration 14, loss = 0.15970043\n",
      "Iteration 15, loss = 0.15965589\n",
      "Iteration 16, loss = 0.15964621\n",
      "Iteration 17, loss = 0.15961846\n",
      "Iteration 18, loss = 0.15961136\n",
      "Iteration 19, loss = 0.15957377\n",
      "Iteration 20, loss = 0.15962471\n",
      "Iteration 21, loss = 0.15956044\n",
      "Iteration 22, loss = 0.15952746\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEm\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\LEm\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\LEm\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\LEm\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\LEm\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\LEm\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\LEm\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\LEm\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\LEm\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.48892907\n",
      "Iteration 2, loss = 0.25166553\n",
      "Iteration 3, loss = 0.20244590\n",
      "Iteration 4, loss = 0.18799445\n",
      "Iteration 5, loss = 0.18136986\n",
      "Iteration 6, loss = 0.17754311\n",
      "Iteration 7, loss = 0.17484031\n",
      "Iteration 8, loss = 0.17321192\n",
      "Iteration 9, loss = 0.17226888\n",
      "Iteration 10, loss = 0.17119937\n",
      "Iteration 11, loss = 0.17055765\n",
      "Iteration 12, loss = 0.17006394\n",
      "Iteration 13, loss = 0.16953533\n",
      "Iteration 14, loss = 0.16930679\n",
      "Iteration 15, loss = 0.16900752\n",
      "Iteration 16, loss = 0.16901378\n",
      "Iteration 17, loss = 0.16877523\n",
      "Iteration 18, loss = 0.16883850\n",
      "Iteration 19, loss = 0.16863304\n",
      "Iteration 20, loss = 0.16852365\n",
      "Iteration 21, loss = 0.16863493\n",
      "Iteration 22, loss = 0.16836198\n",
      "Iteration 23, loss = 0.16844024\n",
      "Iteration 24, loss = 0.16832229\n",
      "Iteration 25, loss = 0.16826388\n",
      "Iteration 26, loss = 0.16828224\n",
      "Iteration 27, loss = 0.16816954\n",
      "Iteration 28, loss = 0.16823874\n",
      "Iteration 29, loss = 0.16823795\n",
      "Iteration 30, loss = 0.16802659\n",
      "Iteration 31, loss = 0.16816288\n",
      "Iteration 32, loss = 0.16821842\n",
      "Iteration 33, loss = 0.16837137\n",
      "Iteration 34, loss = 0.16810136\n",
      "Iteration 35, loss = 0.16819344\n",
      "Iteration 36, loss = 0.16820974\n",
      "Iteration 37, loss = 0.16829844\n",
      "Iteration 38, loss = 0.16799826\n",
      "Iteration 39, loss = 0.16811486\n",
      "Iteration 40, loss = 0.16802784\n",
      "Iteration 41, loss = 0.16802602\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40774467\n",
      "Iteration 2, loss = 0.23575661\n",
      "Iteration 3, loss = 0.19632384\n",
      "Iteration 4, loss = 0.18356234\n",
      "Iteration 5, loss = 0.17780920\n",
      "Iteration 6, loss = 0.17459158\n",
      "Iteration 7, loss = 0.17301083\n",
      "Iteration 8, loss = 0.17175527\n",
      "Iteration 9, loss = 0.17060201\n",
      "Iteration 10, loss = 0.16976347\n",
      "Iteration 11, loss = 0.16916187\n",
      "Iteration 12, loss = 0.16880093\n",
      "Iteration 13, loss = 0.16834109\n",
      "Iteration 14, loss = 0.16824384\n",
      "Iteration 15, loss = 0.16811713\n",
      "Iteration 16, loss = 0.16786555\n",
      "Iteration 17, loss = 0.16782666\n",
      "Iteration 18, loss = 0.16776854\n",
      "Iteration 19, loss = 0.16760872\n",
      "Iteration 20, loss = 0.16748182\n",
      "Iteration 21, loss = 0.16754239\n",
      "Iteration 22, loss = 0.16738183\n",
      "Iteration 23, loss = 0.16741445\n",
      "Iteration 24, loss = 0.16729324\n",
      "Iteration 25, loss = 0.16731677\n",
      "Iteration 26, loss = 0.16721602\n",
      "Iteration 27, loss = 0.16723583\n",
      "Iteration 28, loss = 0.16729458\n",
      "Iteration 29, loss = 0.16732686\n",
      "Iteration 30, loss = 0.16721191\n",
      "Iteration 31, loss = 0.16710469\n",
      "Iteration 32, loss = 0.16706350\n",
      "Iteration 33, loss = 0.16709587\n",
      "Iteration 34, loss = 0.16727938\n",
      "Iteration 35, loss = 0.16714783\n",
      "Iteration 36, loss = 0.16701478\n",
      "Iteration 37, loss = 0.16709416\n",
      "Iteration 38, loss = 0.16699268\n",
      "Iteration 39, loss = 0.16706550\n",
      "Iteration 40, loss = 0.16707734\n",
      "Iteration 41, loss = 0.16708940\n",
      "Iteration 42, loss = 0.16697216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.37119238\n",
      "Iteration 2, loss = 0.23325188\n",
      "Iteration 3, loss = 0.19707503\n",
      "Iteration 4, loss = 0.18503992\n",
      "Iteration 5, loss = 0.17977969\n",
      "Iteration 6, loss = 0.17661953\n",
      "Iteration 7, loss = 0.17479405\n",
      "Iteration 8, loss = 0.17342150\n",
      "Iteration 9, loss = 0.17236340\n",
      "Iteration 10, loss = 0.17147184\n",
      "Iteration 11, loss = 0.17099156\n",
      "Iteration 12, loss = 0.17051620\n",
      "Iteration 13, loss = 0.17034034\n",
      "Iteration 14, loss = 0.17006215\n",
      "Iteration 15, loss = 0.16997636\n",
      "Iteration 16, loss = 0.16966370\n",
      "Iteration 17, loss = 0.16963735\n",
      "Iteration 18, loss = 0.16951474\n",
      "Iteration 19, loss = 0.16957178\n",
      "Iteration 20, loss = 0.16950931\n",
      "Iteration 21, loss = 0.16932333\n",
      "Iteration 22, loss = 0.16918855\n",
      "Iteration 23, loss = 0.16920544\n",
      "Iteration 24, loss = 0.16901588\n",
      "Iteration 25, loss = 0.16911986\n",
      "Iteration 26, loss = 0.16906293\n",
      "Iteration 27, loss = 0.16905580\n",
      "Iteration 28, loss = 0.16892609\n",
      "Iteration 29, loss = 0.16892789\n",
      "Iteration 30, loss = 0.16899630\n",
      "Iteration 31, loss = 0.16892414\n",
      "Iteration 32, loss = 0.16894479\n",
      "Iteration 33, loss = 0.16889730\n",
      "Iteration 34, loss = 0.16878126\n",
      "Iteration 35, loss = 0.16887357\n",
      "Iteration 36, loss = 0.16877341\n",
      "Iteration 37, loss = 0.16880745\n",
      "Iteration 38, loss = 0.16882094\n",
      "Iteration 39, loss = 0.16877995\n",
      "Iteration 40, loss = 0.16863914\n",
      "Iteration 41, loss = 0.16875288\n",
      "Iteration 42, loss = 0.16871678\n",
      "Iteration 43, loss = 0.16881006\n",
      "Iteration 44, loss = 0.16877048\n",
      "Iteration 45, loss = 0.16879067\n",
      "Iteration 46, loss = 0.16867355\n",
      "Iteration 47, loss = 0.16868093\n",
      "Iteration 48, loss = 0.16882907\n",
      "Iteration 49, loss = 0.16864801\n",
      "Iteration 50, loss = 0.16869462\n",
      "Iteration 51, loss = 0.16871026\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48485259\n",
      "Iteration 2, loss = 0.26705667\n",
      "Iteration 3, loss = 0.20833848\n",
      "Iteration 4, loss = 0.18948665\n",
      "Iteration 5, loss = 0.18154200\n",
      "Iteration 6, loss = 0.17740234\n",
      "Iteration 7, loss = 0.17486414\n",
      "Iteration 8, loss = 0.17284289\n",
      "Iteration 9, loss = 0.17176974\n",
      "Iteration 10, loss = 0.17065829\n",
      "Iteration 11, loss = 0.16998781\n",
      "Iteration 12, loss = 0.16946284\n",
      "Iteration 13, loss = 0.16921698\n",
      "Iteration 14, loss = 0.16889212\n",
      "Iteration 15, loss = 0.16859596\n",
      "Iteration 16, loss = 0.16845974\n",
      "Iteration 17, loss = 0.16840697\n",
      "Iteration 18, loss = 0.16836409\n",
      "Iteration 19, loss = 0.16828213\n",
      "Iteration 20, loss = 0.16813119\n",
      "Iteration 21, loss = 0.16800244\n",
      "Iteration 22, loss = 0.16796954\n",
      "Iteration 23, loss = 0.16800481\n",
      "Iteration 24, loss = 0.16802519\n",
      "Iteration 25, loss = 0.16794227\n",
      "Iteration 26, loss = 0.16790702\n",
      "Iteration 27, loss = 0.16773063\n",
      "Iteration 28, loss = 0.16785001\n",
      "Iteration 29, loss = 0.16769915\n",
      "Iteration 30, loss = 0.16767352\n",
      "Iteration 31, loss = 0.16763481\n",
      "Iteration 32, loss = 0.16758830\n",
      "Iteration 33, loss = 0.16782005\n",
      "Iteration 34, loss = 0.16766232\n",
      "Iteration 35, loss = 0.16758715\n",
      "Iteration 36, loss = 0.16763142\n",
      "Iteration 37, loss = 0.16759924\n",
      "Iteration 38, loss = 0.16750812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45184388\n",
      "Iteration 2, loss = 0.26075837\n",
      "Iteration 3, loss = 0.21181514\n",
      "Iteration 4, loss = 0.19401054\n",
      "Iteration 5, loss = 0.18497037\n",
      "Iteration 6, loss = 0.18006047\n",
      "Iteration 7, loss = 0.17676484\n",
      "Iteration 8, loss = 0.17447415\n",
      "Iteration 9, loss = 0.17289412\n",
      "Iteration 10, loss = 0.17177751\n",
      "Iteration 11, loss = 0.17072472\n",
      "Iteration 12, loss = 0.17022291\n",
      "Iteration 13, loss = 0.16962331\n",
      "Iteration 14, loss = 0.16930474\n",
      "Iteration 15, loss = 0.16886634\n",
      "Iteration 16, loss = 0.16859347\n",
      "Iteration 17, loss = 0.16836746\n",
      "Iteration 18, loss = 0.16830134\n",
      "Iteration 19, loss = 0.16808344\n",
      "Iteration 20, loss = 0.16803525\n",
      "Iteration 21, loss = 0.16797420\n",
      "Iteration 22, loss = 0.16775684\n",
      "Iteration 23, loss = 0.16771924\n",
      "Iteration 24, loss = 0.16758794\n",
      "Iteration 25, loss = 0.16749703\n",
      "Iteration 26, loss = 0.16728664\n",
      "Iteration 27, loss = 0.16752444\n",
      "Iteration 28, loss = 0.16737712\n",
      "Iteration 29, loss = 0.16739390\n",
      "Iteration 30, loss = 0.16715306\n",
      "Iteration 31, loss = 0.16717241\n",
      "Iteration 32, loss = 0.16727108\n",
      "Iteration 33, loss = 0.16715481\n",
      "Iteration 34, loss = 0.16718309\n",
      "Iteration 35, loss = 0.16699755\n",
      "Iteration 36, loss = 0.16693060\n",
      "Iteration 37, loss = 0.16698798\n",
      "Iteration 38, loss = 0.16713292\n",
      "Iteration 39, loss = 0.16690967\n",
      "Iteration 40, loss = 0.16704721\n",
      "Iteration 41, loss = 0.16698122\n",
      "Iteration 42, loss = 0.16699685\n",
      "Iteration 43, loss = 0.16680964\n",
      "Iteration 44, loss = 0.16698114\n",
      "Iteration 45, loss = 0.16687054\n",
      "Iteration 46, loss = 0.16678852\n",
      "Iteration 47, loss = 0.16672873\n",
      "Iteration 48, loss = 0.16691552\n",
      "Iteration 49, loss = 0.16681188\n",
      "Iteration 50, loss = 0.16694322\n",
      "Iteration 51, loss = 0.16666273\n",
      "Iteration 52, loss = 0.16681396\n",
      "Iteration 53, loss = 0.16673067\n",
      "Iteration 54, loss = 0.16677288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39442659\n",
      "Iteration 2, loss = 0.24122247\n",
      "Iteration 3, loss = 0.20297112\n",
      "Iteration 4, loss = 0.19050488\n",
      "Iteration 5, loss = 0.18456645\n",
      "Iteration 6, loss = 0.18087264\n",
      "Iteration 7, loss = 0.17854079\n",
      "Iteration 8, loss = 0.17677251\n",
      "Iteration 9, loss = 0.17568654\n",
      "Iteration 10, loss = 0.17477822\n",
      "Iteration 11, loss = 0.17415676\n",
      "Iteration 12, loss = 0.17363624\n",
      "Iteration 13, loss = 0.17319183\n",
      "Iteration 14, loss = 0.17272385\n",
      "Iteration 15, loss = 0.17269493\n",
      "Iteration 16, loss = 0.17256386\n",
      "Iteration 17, loss = 0.17227955\n",
      "Iteration 18, loss = 0.17235145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.17211451\n",
      "Iteration 20, loss = 0.17177975\n",
      "Iteration 21, loss = 0.17190084\n",
      "Iteration 22, loss = 0.17168785\n",
      "Iteration 23, loss = 0.17158649\n",
      "Iteration 24, loss = 0.17164491\n",
      "Iteration 25, loss = 0.17143937\n",
      "Iteration 26, loss = 0.17148690\n",
      "Iteration 27, loss = 0.17157937\n",
      "Iteration 28, loss = 0.17126049\n",
      "Iteration 29, loss = 0.17128867\n",
      "Iteration 30, loss = 0.17128537\n",
      "Iteration 31, loss = 0.17120853\n",
      "Iteration 32, loss = 0.17125408\n",
      "Iteration 33, loss = 0.17126612\n",
      "Iteration 34, loss = 0.17104033\n",
      "Iteration 35, loss = 0.17128975\n",
      "Iteration 36, loss = 0.17105420\n",
      "Iteration 37, loss = 0.17100883\n",
      "Iteration 38, loss = 0.17103718\n",
      "Iteration 39, loss = 0.17107349\n",
      "Iteration 40, loss = 0.17110511\n",
      "Iteration 41, loss = 0.17100747\n",
      "Iteration 42, loss = 0.17124864\n",
      "Iteration 43, loss = 0.17091055\n",
      "Iteration 44, loss = 0.17116297\n",
      "Iteration 45, loss = 0.17099877\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40897591\n",
      "Iteration 2, loss = 0.20491489\n",
      "Iteration 3, loss = 0.16061433\n",
      "Iteration 4, loss = 0.14736129\n",
      "Iteration 5, loss = 0.14157138\n",
      "Iteration 6, loss = 0.13868293\n",
      "Iteration 7, loss = 0.13655832\n",
      "Iteration 8, loss = 0.13509309\n",
      "Iteration 9, loss = 0.13391833\n",
      "Iteration 10, loss = 0.13316457\n",
      "Iteration 11, loss = 0.13213131\n",
      "Iteration 12, loss = 0.13124864\n",
      "Iteration 13, loss = 0.13053236\n",
      "Iteration 14, loss = 0.12978765\n",
      "Iteration 15, loss = 0.12925183\n",
      "Iteration 16, loss = 0.12840206\n",
      "Iteration 17, loss = 0.12771913\n",
      "Iteration 18, loss = 0.12717259\n",
      "Iteration 19, loss = 0.12665013\n",
      "Iteration 20, loss = 0.12594822\n",
      "Iteration 21, loss = 0.12540389\n",
      "Iteration 22, loss = 0.12457004\n",
      "Iteration 23, loss = 0.12425874\n",
      "Iteration 24, loss = 0.12360686\n",
      "Iteration 25, loss = 0.12308933\n",
      "Iteration 26, loss = 0.12256678\n",
      "Iteration 27, loss = 0.12186461\n",
      "Iteration 28, loss = 0.12140155\n",
      "Iteration 29, loss = 0.12092938\n",
      "Iteration 30, loss = 0.12063933\n",
      "Iteration 31, loss = 0.11993412\n",
      "Iteration 32, loss = 0.11956444\n",
      "Iteration 33, loss = 0.11917599\n",
      "Iteration 34, loss = 0.11883062\n",
      "Iteration 35, loss = 0.11825976\n",
      "Iteration 36, loss = 0.11807117\n",
      "Iteration 37, loss = 0.11757102\n",
      "Iteration 38, loss = 0.11741869\n",
      "Iteration 39, loss = 0.11712029\n",
      "Iteration 40, loss = 0.11693058\n",
      "Iteration 41, loss = 0.11679223\n",
      "Iteration 42, loss = 0.11649232\n",
      "Iteration 43, loss = 0.11580996\n",
      "Iteration 44, loss = 0.11604052\n",
      "Iteration 45, loss = 0.11552174\n",
      "Iteration 46, loss = 0.11539918\n",
      "Iteration 47, loss = 0.11529089\n",
      "Iteration 48, loss = 0.11488205\n",
      "Iteration 49, loss = 0.11458887\n",
      "Iteration 50, loss = 0.11463176\n",
      "Iteration 51, loss = 0.11425597\n",
      "Iteration 52, loss = 0.11419263\n",
      "Iteration 53, loss = 0.11393306\n",
      "Iteration 54, loss = 0.11379477\n",
      "Iteration 55, loss = 0.11351327\n",
      "Iteration 56, loss = 0.11350753\n",
      "Iteration 57, loss = 0.11329965\n",
      "Iteration 58, loss = 0.11305880\n",
      "Iteration 59, loss = 0.11287142\n",
      "Iteration 60, loss = 0.11276404\n",
      "Iteration 61, loss = 0.11247335\n",
      "Iteration 62, loss = 0.11242985\n",
      "Iteration 63, loss = 0.11234262\n",
      "Iteration 64, loss = 0.11209710\n",
      "Iteration 65, loss = 0.11207833\n",
      "Iteration 66, loss = 0.11188356\n",
      "Iteration 67, loss = 0.11179314\n",
      "Iteration 68, loss = 0.11176692\n",
      "Iteration 69, loss = 0.11156732\n",
      "Iteration 70, loss = 0.11123122\n",
      "Iteration 71, loss = 0.11127696\n",
      "Iteration 72, loss = 0.11094830\n",
      "Iteration 73, loss = 0.11081313\n",
      "Iteration 74, loss = 0.11056474\n",
      "Iteration 75, loss = 0.11045096\n",
      "Iteration 76, loss = 0.11048276\n",
      "Iteration 77, loss = 0.11023710\n",
      "Iteration 78, loss = 0.11020770\n",
      "Iteration 79, loss = 0.10999733\n",
      "Iteration 80, loss = 0.10977313\n",
      "Iteration 81, loss = 0.10965476\n",
      "Iteration 82, loss = 0.10962659\n",
      "Iteration 83, loss = 0.10937868\n",
      "Iteration 84, loss = 0.10947231\n",
      "Iteration 85, loss = 0.10909863\n",
      "Iteration 86, loss = 0.10911457\n",
      "Iteration 87, loss = 0.10906718\n",
      "Iteration 88, loss = 0.10868876\n",
      "Iteration 89, loss = 0.10864203\n",
      "Iteration 90, loss = 0.10883751\n",
      "Iteration 91, loss = 0.10840840\n",
      "Iteration 92, loss = 0.10842025\n",
      "Iteration 93, loss = 0.10816441\n",
      "Iteration 94, loss = 0.10789247\n",
      "Iteration 95, loss = 0.10785869\n",
      "Iteration 96, loss = 0.10765956\n",
      "Iteration 97, loss = 0.10786537\n",
      "Iteration 98, loss = 0.10761741\n",
      "Iteration 99, loss = 0.10751390\n",
      "Iteration 100, loss = 0.10725950\n",
      "Iteration 101, loss = 0.10712434\n",
      "Iteration 102, loss = 0.10713767\n",
      "Iteration 103, loss = 0.10713726\n",
      "Iteration 104, loss = 0.10699971\n",
      "Iteration 105, loss = 0.10672956\n",
      "Iteration 106, loss = 0.10635144\n",
      "Iteration 107, loss = 0.10668503\n",
      "Iteration 108, loss = 0.10650188\n",
      "Iteration 109, loss = 0.10630787\n",
      "Iteration 110, loss = 0.10606272\n",
      "Iteration 111, loss = 0.10621902\n",
      "Iteration 112, loss = 0.10624197\n",
      "Iteration 113, loss = 0.10606449\n",
      "Iteration 114, loss = 0.10586322\n",
      "Iteration 115, loss = 0.10569439\n",
      "Iteration 116, loss = 0.10566697\n",
      "Iteration 117, loss = 0.10537140\n",
      "Iteration 118, loss = 0.10539460\n",
      "Iteration 119, loss = 0.10537168\n",
      "Iteration 120, loss = 0.10532533\n",
      "Iteration 121, loss = 0.10509790\n",
      "Iteration 122, loss = 0.10517885\n",
      "Iteration 123, loss = 0.10491513\n",
      "Iteration 124, loss = 0.10496717\n",
      "Iteration 125, loss = 0.10462477\n",
      "Iteration 126, loss = 0.10434515\n",
      "Iteration 127, loss = 0.10453951\n",
      "Iteration 128, loss = 0.10476719\n",
      "Iteration 129, loss = 0.10431263\n",
      "Iteration 130, loss = 0.10438334\n",
      "Iteration 131, loss = 0.10427270\n",
      "Iteration 132, loss = 0.10427452\n",
      "Iteration 133, loss = 0.10391870\n",
      "Iteration 134, loss = 0.10394357\n",
      "Iteration 135, loss = 0.10370141\n",
      "Iteration 136, loss = 0.10377483\n",
      "Iteration 137, loss = 0.10378036\n",
      "Iteration 138, loss = 0.10368599\n",
      "Iteration 139, loss = 0.10348320\n",
      "Iteration 140, loss = 0.10327463\n",
      "Iteration 141, loss = 0.10351367\n",
      "Iteration 142, loss = 0.10315743\n",
      "Iteration 143, loss = 0.10320340\n",
      "Iteration 144, loss = 0.10306656\n",
      "Iteration 145, loss = 0.10280504\n",
      "Iteration 146, loss = 0.10308853\n",
      "Iteration 147, loss = 0.10289381\n",
      "Iteration 148, loss = 0.10286253\n",
      "Iteration 149, loss = 0.10285425\n",
      "Iteration 150, loss = 0.10243944\n",
      "Iteration 151, loss = 0.10247167\n",
      "Iteration 152, loss = 0.10264604\n",
      "Iteration 153, loss = 0.10267425\n",
      "Iteration 154, loss = 0.10220715\n",
      "Iteration 155, loss = 0.10220601\n",
      "Iteration 156, loss = 0.10215097\n",
      "Iteration 157, loss = 0.10219035\n",
      "Iteration 158, loss = 0.10194229\n",
      "Iteration 159, loss = 0.10213069\n",
      "Iteration 160, loss = 0.10197322\n",
      "Iteration 161, loss = 0.10178664\n",
      "Iteration 162, loss = 0.10163663\n",
      "Iteration 163, loss = 0.10146818\n",
      "Iteration 164, loss = 0.10165625\n",
      "Iteration 165, loss = 0.10164161\n",
      "Iteration 166, loss = 0.10161015\n",
      "Iteration 167, loss = 0.10124129\n",
      "Iteration 168, loss = 0.10138870\n",
      "Iteration 169, loss = 0.10150982\n",
      "Iteration 170, loss = 0.10121665\n",
      "Iteration 171, loss = 0.10111842\n",
      "Iteration 172, loss = 0.10098036\n",
      "Iteration 173, loss = 0.10106474\n",
      "Iteration 174, loss = 0.10115379\n",
      "Iteration 175, loss = 0.10083411\n",
      "Iteration 176, loss = 0.10086979\n",
      "Iteration 177, loss = 0.10089412\n",
      "Iteration 178, loss = 0.10126172\n",
      "Iteration 179, loss = 0.10065022\n",
      "Iteration 180, loss = 0.10050054\n",
      "Iteration 181, loss = 0.10060114\n",
      "Iteration 182, loss = 0.10031970\n",
      "Iteration 183, loss = 0.10063114\n",
      "Iteration 184, loss = 0.10026481\n",
      "Iteration 185, loss = 0.10034847\n",
      "Iteration 186, loss = 0.10029146\n",
      "Iteration 187, loss = 0.10031472\n",
      "Iteration 188, loss = 0.10032889\n",
      "Iteration 189, loss = 0.10007487\n",
      "Iteration 190, loss = 0.10025459\n",
      "Iteration 191, loss = 0.10006748\n",
      "Iteration 192, loss = 0.10000433\n",
      "Iteration 193, loss = 0.09973964\n",
      "Iteration 194, loss = 0.10005653\n",
      "Iteration 195, loss = 0.09960111\n",
      "Iteration 196, loss = 0.09993995\n",
      "Iteration 197, loss = 0.09970206\n",
      "Iteration 198, loss = 0.09951138\n",
      "Iteration 199, loss = 0.09940953\n",
      "Iteration 200, loss = 0.09954660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEm\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.43356551\n",
      "Iteration 2, loss = 0.23849093\n",
      "Iteration 3, loss = 0.17866213\n",
      "Iteration 4, loss = 0.15609459\n",
      "Iteration 5, loss = 0.14613023\n",
      "Iteration 6, loss = 0.14082568\n",
      "Iteration 7, loss = 0.13746849\n",
      "Iteration 8, loss = 0.13516803\n",
      "Iteration 9, loss = 0.13347502\n",
      "Iteration 10, loss = 0.13197186\n",
      "Iteration 11, loss = 0.13087770\n",
      "Iteration 12, loss = 0.12977505\n",
      "Iteration 13, loss = 0.12873746\n",
      "Iteration 14, loss = 0.12810431\n",
      "Iteration 15, loss = 0.12716588\n",
      "Iteration 16, loss = 0.12635059\n",
      "Iteration 17, loss = 0.12572446\n",
      "Iteration 18, loss = 0.12501772\n",
      "Iteration 19, loss = 0.12452491\n",
      "Iteration 20, loss = 0.12384954\n",
      "Iteration 21, loss = 0.12320323\n",
      "Iteration 22, loss = 0.12283299\n",
      "Iteration 23, loss = 0.12246799\n",
      "Iteration 24, loss = 0.12175186\n",
      "Iteration 25, loss = 0.12144506\n"
     ]
    }
   ],
   "source": [
    "clf_mlp_models = []\n",
    "pred_release_training_MLP = []\n",
    "pred_release_testing_MLP = []\n",
    "pred_release_prob_MLP = []\n",
    "\n",
    "confusion_matrixes_training = []\n",
    "confusion_matrixes_test = []\n",
    "#feature_importances = []\n",
    "\n",
    "f1_scores = []\n",
    "ROC_curves = []\n",
    "CV_scores = []\n",
    "R2_scores = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "roc_auc_scores = []\n",
    "\n",
    "\n",
    "\n",
    "n = 0\n",
    "for i in feature_combinations:\n",
    "    \n",
    "    X_train = train_X[i]\n",
    "    X_test = validation_X[i]\n",
    "\n",
    "    AUTO_SCALING = True\n",
    "    if AUTO_SCALING:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_training = scaler.transform(X_train)\n",
    "        X_testing = scaler.transform(X_test)\n",
    "       \n",
    "    #nnetwork = MLPClassifier(max_iter=1000)    \n",
    "   \n",
    "    clf_mlp = MLPClassifier(solver = best_param_grid[n][\"solver\"], hidden_layer_sizes = best_param_grid[n][\"hidden_layer_sizes\"], alpha = best_param_grid[n][\"alpha\"], max_iter=200,verbose=3)\n",
    "    clf_mlp.fit(X_training, train_y)\n",
    "    clf_mlp_models.append(clf_mlp)\n",
    "    \n",
    "    pred_release_at_training_MLP = clf_mlp.predict(X_training)\n",
    "    pred_release_training_MLP.append(pred_release_at_training_MLP)\n",
    "    \n",
    "    pred_prob_MLP = clf_mlp.predict_proba(X_testing)\n",
    "    pred_release_prob_MLP.append(pred_prob_MLP)\n",
    "    \n",
    "    pred_release_at_validation_MLP = clf_mlp.predict(X_testing)\n",
    "    pred_release_testing_MLP.append(pred_release_at_validation_MLP)\n",
    "    \n",
    "    c_m = confusion_matrix(train_y,pred_release_at_training_MLP)\n",
    "    confusion_matrixes_training.append(c_m)\n",
    "    \n",
    "    c_m_v = confusion_matrix(validation_y,pred_release_at_validation_MLP)\n",
    "    confusion_matrixes_test.append(c_m_v)\n",
    "    \n",
    "    f1 = f1_score(validation_y, pred_release_at_validation_MLP, average='macro')\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    CV = cross_val_score(estimator= clf_mlp, X=X_training, y=train_y)\n",
    "    CV_scores.append(CV)\n",
    "    \n",
    "    CLF_ROC = plot_roc_curve(clf_mlp, X_testing, validation_y, color = 'r')\n",
    "    ROC_curves.append(CLF_ROC)\n",
    "    plt.close()\n",
    "    \n",
    "    r2 = r2_score(validation_y, pred_release_at_validation_MLP)\n",
    "    R2_scores.append(r2)\n",
    "    \n",
    "    acc = accuracy_score(validation_y, pred_release_at_validation_MLP)\n",
    "    accuracy_scores.append(acc)\n",
    "    \n",
    "    precision = precision_score(validation_y, pred_release_at_validation_MLP)\n",
    "    precision_scores.append(precision)\n",
    "    \n",
    "    recall = recall_score(validation_y, pred_release_at_validation_MLP)\n",
    "    recall_scores.append(recall)\n",
    "    \n",
    "    roc_auc = roc_auc_score(validation_y, pred_release_at_validation_MLP)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    \n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final models and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLP_p8_Emhjellen2.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save models\n",
    "joblib.dump(clf_mlp_models[0], \"MLP_p1_Emhjellen2.joblib\",compress=3)\n",
    "joblib.dump(clf_mlp_models[1], \"MLP_p2_Emhjellen2.joblib\",compress=3)\n",
    "joblib.dump(clf_mlp_models[2], \"MLP_p3_Emhjellen2.joblib\",compress=3)\n",
    "joblib.dump(clf_mlp_models[3], \"MLP_p4_Emhjellen2.joblib\",compress=3)\n",
    "joblib.dump(clf_mlp_models[4], \"MLP_p5_Emhjellen2.joblib\",compress=3)\n",
    "joblib.dump(clf_mlp_models[5], \"MLP_p6_Emhjellen2.joblib\",compress=3)\n",
    "joblib.dump(clf_mlp_models[6], \"MLP_p7_Emhjellen2.joblib\",compress=3)\n",
    "joblib.dump(clf_mlp_models[7], \"MLP_p8_Emhjellen2.joblib\",compress=3)\n",
    "\n",
    "# ../saved_models/RF_p6_Emhjellen2.joblib\",compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of confusion matrixes from training\n",
    "n = 0;\n",
    "for i in confusion_matrixes_training:\n",
    "    n += 1\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                i.flatten()]\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     i.flatten()/np.sum(i)]\n",
    "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
    "    zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure()\n",
    "    plot = sns.heatmap(i, annot=labels, fmt='', cmap='Blues')\n",
    "    plt.savefig('saved_figures_2/confusion_matrix_MLP_feature_combination_training'+ 'p'+ str(n) +'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of confusion matrixes from test\n",
    "n = 0;\n",
    "for i in confusion_matrixes_test:\n",
    "    n += 1\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                i.flatten()]\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     i.flatten()/np.sum(i)]\n",
    "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
    "    zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure()\n",
    "    plot = sns.heatmap(i, annot=labels, fmt='', cmap='Blues')\n",
    "    plt.savefig('saved_figures_2/confusion_matrix_MLP_feature_combination_validation'+ 'p'+ str(n) +'.png')\n",
    "    plt.close()\n",
    "    \n",
    "    #plt.savefig('saved_figures/confusion_matrix_RF_feature_combination_validation'+ 'p'+ str(n) +'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bar plot of f1 scores\n",
    "\n",
    "fig = sns.barplot(x=np.arange(1,9), y=f1_scores,palette=\"Blues_d\")\n",
    "    \n",
    "plt.xlabel('Feature Combination')\n",
    "plt.ylabel('F1 score')\n",
    "plt.title(\"\")\n",
    "ticks = np.arange(0, 1, 0.05)\n",
    "fig.set_yticks(ticks, minor=True)\n",
    "fig.grid(which='both',axis='y',linestyle='dashed')\n",
    "plt.rc('axes', axisbelow=True)\n",
    "plt.savefig('saved_figures_2/f1_scores_MLP.png')\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot of ROC curves\n",
    "\n",
    "ax = plt.gca()\n",
    "n = 0\n",
    "for i in ROC_curves:\n",
    "    n += 1\n",
    "    i.plot(ax=ax, alpha=0.8, label = 'Multilayer Perceptron '+ 'pc'+ str(n))\n",
    "plt.savefig('saved_figures_2/ROC_curve_feature_combinations_MLP_2.png')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_RF = pd.DataFrame(CV_scores)\n",
    "cv_scores_RF.to_excel('MLP_CV_scores_training2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_RF = pd.DataFrame(f1_scores)\n",
    "f1_scores_RF.to_excel('MLP_F1_scores2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores_RF = pd.DataFrame(accuracy_scores)\n",
    "accuracy_scores_RF.to_excel('MLP_accuracy_scores2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_scores_RF = pd.DataFrame(precision_scores)\n",
    "precision_scores_RF.to_excel('MLP_precision_scores2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores_RF = pd.DataFrame(recall_scores)\n",
    "recall_scores_RF.to_excel('MLP_recall_scores2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_scores_RF = pd.DataFrame(R2_scores)\n",
    "R2_scores_RF.to_excel('MLP_R2_scores2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_scores = pd.DataFrame(roc_auc_scores)\n",
    "roc_auc_scores.to_excel('MLP_roc_auc_scores.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_y = pd.read_excel('ML_validation_target.xlsx')\n",
    "\n",
    "df_coor = validation_y[['POINT_X', 'POINT_Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-361dda0d81ad>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_coor['MLP_prob_p1'] = pred_release_prob_MLP[0][:,1]\n"
     ]
    }
   ],
   "source": [
    "# extracting the values that are probability for rockfall, class 1\n",
    "\n",
    "df_coor['MLP_prob_p1'] = pred_release_prob_MLP[0][:,1]\n",
    "df_coor['MLP_prob_p2'] = pred_release_prob_MLP[1][:,1]\n",
    "df_coor['MLP_prob_p3'] = pred_release_prob_MLP[2][:,1]\n",
    "df_coor['MLP_prob_p4'] = pred_release_prob_MLP[3][:,1]\n",
    "df_coor['MLP_prob_p5'] = pred_release_prob_MLP[4][:,1]\n",
    "df_coor['MLP_prob_p6'] = pred_release_prob_MLP[5][:,1]\n",
    "df_coor['MLP_prob_p7'] = pred_release_prob_MLP[6][:,1]\n",
    "df_coor['MLP_prob_p8'] = pred_release_prob_MLP[7][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coor['MLP_prediction_p1'] = pred_release_testing_MLP[0]\n",
    "df_coor['MLP_prediction_p2'] = pred_release_testing_MLP[1]\n",
    "df_coor['MLP_prediction_p3'] = pred_release_testing_MLP[2]\n",
    "df_coor['MLP_prediction_p4'] = pred_release_testing_MLP[3]\n",
    "df_coor['MLP_prediction_p5'] = pred_release_testing_MLP[4]\n",
    "df_coor['MLP_prediction_p6'] = pred_release_testing_MLP[5]\n",
    "df_coor['MLP_prediction_p7'] = pred_release_testing_MLP[6]\n",
    "df_coor['MLP_prediction_p8'] = pred_release_testing_MLP[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coor['ReleaseArea'] = validation_y['ReleaseArea'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = []\n",
    "for row in range(0,len(df_coor)):\n",
    "    if df_coor['ReleaseArea'].iloc[row] == 1 and df_coor['MLP_prediction_p1'].iloc[row] == 1:\n",
    "        result1.append('TP')\n",
    "    elif df_coor['ReleaseArea'].iloc[row] == 0 and df_coor['MLP_prediction_p1'].iloc[row] == 0:\n",
    "        result1.append('TN')\n",
    "    elif df_coor['MLP_prediction_p1'].iloc[row] == 1 and df_coor['ReleaseArea'].iloc[row] == 0:\n",
    "        result1.append('FP')\n",
    "    elif df_coor['MLP_prediction_p1'].iloc[row] == 0 and df_coor['ReleaseArea'].iloc[row] == 1:\n",
    "        result1.append('FN')\n",
    "    else:\n",
    "        print('NaN')\n",
    "        \n",
    "        \n",
    "df_coor['result_p1'] = result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = []\n",
    "for row in range(0,len(df_coor)):\n",
    "    if df_coor['ReleaseArea'].iloc[row] == 1 and df_coor['MLP_prediction_p2'].iloc[row] == 1:\n",
    "        result2.append('TP')\n",
    "    elif df_coor['ReleaseArea'].iloc[row] == 0 and df_coor['MLP_prediction_p2'].iloc[row] == 0:\n",
    "        result2.append('TN')\n",
    "    elif df_coor['MLP_prediction_p2'].iloc[row] == 1 and df_coor['ReleaseArea'].iloc[row] == 0:\n",
    "        result2.append('FP')\n",
    "    elif df_coor['MLP_prediction_p2'].iloc[row] == 0 and df_coor['ReleaseArea'].iloc[row] == 1:\n",
    "        result2.append('FN')\n",
    "    else:\n",
    "        print('NaN')\n",
    "        \n",
    "        \n",
    "df_coor['result_p2'] = result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = []\n",
    "for row in range(0,len(df_coor)):\n",
    "    if df_coor['ReleaseArea'].iloc[row] == 1 and df_coor['MLP_prediction_p3'].iloc[row] == 1:\n",
    "        result3.append('TP')\n",
    "    elif df_coor['ReleaseArea'].iloc[row] == 0 and df_coor['MLP_prediction_p3'].iloc[row] == 0:\n",
    "        result3.append('TN')\n",
    "    elif df_coor['MLP_prediction_p3'].iloc[row] == 1 and df_coor['ReleaseArea'].iloc[row] == 0:\n",
    "        result3.append('FP')\n",
    "    elif df_coor['MLP_prediction_p3'].iloc[row] == 0 and df_coor['ReleaseArea'].iloc[row] == 1:\n",
    "        result3.append('FN')\n",
    "    else:\n",
    "        print('NaN')\n",
    "        \n",
    "        \n",
    "df_coor['result_p3'] = result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = []\n",
    "for row in range(0,len(df_coor)):\n",
    "    if df_coor['ReleaseArea'].iloc[row] == 1 and df_coor['MLP_prediction_p4'].iloc[row] == 1:\n",
    "        result4.append('TP')\n",
    "    elif df_coor['ReleaseArea'].iloc[row] == 0 and df_coor['MLP_prediction_p4'].iloc[row] == 0:\n",
    "        result4.append('TN')\n",
    "    elif df_coor['MLP_prediction_p4'].iloc[row] == 1 and df_coor['ReleaseArea'].iloc[row] == 0:\n",
    "        result4.append('FP')\n",
    "    elif df_coor['MLP_prediction_p4'].iloc[row] == 0 and df_coor['ReleaseArea'].iloc[row] == 1:\n",
    "        result4.append('FN')\n",
    "    else:\n",
    "        print('NaN')\n",
    "        \n",
    "        \n",
    "df_coor['result_p4'] = result4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result5 = []\n",
    "for row in range(0,len(df_coor)):\n",
    "    if df_coor['ReleaseArea'].iloc[row] == 1 and df_coor['MLP_prediction_p5'].iloc[row] == 1:\n",
    "        result5.append('TP')\n",
    "    elif df_coor['ReleaseArea'].iloc[row] == 0 and df_coor['MLP_prediction_p5'].iloc[row] == 0:\n",
    "        result5.append('TN')\n",
    "    elif df_coor['MLP_prediction_p5'].iloc[row] == 1 and df_coor['ReleaseArea'].iloc[row] == 0:\n",
    "        result5.append('FP')\n",
    "    elif df_coor['MLP_prediction_p5'].iloc[row] == 0 and df_coor['ReleaseArea'].iloc[row] == 1:\n",
    "        result5.append('FN')\n",
    "    else:\n",
    "        print('NaN')\n",
    "        \n",
    "        \n",
    "df_coor['result_p5'] = result5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result6 = []\n",
    "for row in range(0,len(df_coor)):\n",
    "    if df_coor['ReleaseArea'].iloc[row] == 1 and df_coor['MLP_prediction_p6'].iloc[row] == 1:\n",
    "        result6.append('TP')\n",
    "    elif df_coor['ReleaseArea'].iloc[row] == 0 and df_coor['MLP_prediction_p6'].iloc[row] == 0:\n",
    "        result6.append('TN')\n",
    "    elif df_coor['MLP_prediction_p6'].iloc[row] == 1 and df_coor['ReleaseArea'].iloc[row] == 0:\n",
    "        result6.append('FP')\n",
    "    elif df_coor['MLP_prediction_p6'].iloc[row] == 0 and df_coor['ReleaseArea'].iloc[row] == 1:\n",
    "        result6.append('FN')\n",
    "    else:\n",
    "        print('NaN')\n",
    "        \n",
    "        \n",
    "df_coor['result_p6'] = result6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result7 = []\n",
    "for row in range(0,len(df_coor)):\n",
    "    if df_coor['ReleaseArea'].iloc[row] == 1 and df_coor['MLP_prediction_p7'].iloc[row] == 1:\n",
    "        result7.append('TP')\n",
    "    elif df_coor['ReleaseArea'].iloc[row] == 0 and df_coor['MLP_prediction_p7'].iloc[row] == 0:\n",
    "        result7.append('TN')\n",
    "    elif df_coor['MLP_prediction_p7'].iloc[row] == 1 and df_coor['ReleaseArea'].iloc[row] == 0:\n",
    "        result7.append('FP')\n",
    "    elif df_coor['MLP_prediction_p7'].iloc[row] == 0 and df_coor['ReleaseArea'].iloc[row] == 1:\n",
    "        result7.append('FN')\n",
    "    else:\n",
    "        print('NaN')\n",
    "        \n",
    "        \n",
    "df_coor['result_p7'] = result7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result8 = []\n",
    "for row in range(0,len(df_coor)):\n",
    "    if df_coor['ReleaseArea'].iloc[row] == 1 and df_coor['MLP_prediction_p8'].iloc[row] == 1:\n",
    "        result8.append('TP')\n",
    "    elif df_coor['ReleaseArea'].iloc[row] == 0 and df_coor['MLP_prediction_p8'].iloc[row] == 0:\n",
    "        result8.append('TN')\n",
    "    elif df_coor['MLP_prediction_p8'].iloc[row] == 1 and df_coor['ReleaseArea'].iloc[row] == 0:\n",
    "        result8.append('FP')\n",
    "    elif df_coor['MLP_prediction_p8'].iloc[row] == 0 and df_coor['ReleaseArea'].iloc[row] == 1:\n",
    "        result8.append('FN')\n",
    "    else:\n",
    "        print('NaN')\n",
    "        \n",
    "        \n",
    "df_coor['result_p8'] = result8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coor.to_excel('prediction_results_validation_MLP2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
